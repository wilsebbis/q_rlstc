"""Load RLSTCcode-format pickle data files.

Provides functions to load the pre-processed T-Drive (and GeoLife) pickle
files that were generated by RLSTCcode's ``preprocessing.py`` and
``initcenters.py``.  These are the canonical datasets used for training
and evaluation.

Pickle file formats:
    - ``Tdrive_norm_traj``: List of ``Traj(points, size, ts, te, traj_id)``
    - ``tdrive_clustercenter``: ``[(overall_sim, overall_sim, cluster_dict)]``
    - ``traclus_subtrajs``: List of ``Traj`` objects (MDL-simplified sub-trajectories)
    - ``tdrive_testsetN``: Same format as norm_traj (held-out evaluation sets)
"""

import pickle
import sys
from pathlib import Path
from typing import List, Tuple, Dict, Any, Optional

import numpy as np

from ..data.synthetic import Point, Trajectory


# ---------------------------------------------------------------------------
# RLSTCcode path for unpickling its classes
# ---------------------------------------------------------------------------
_RLSTC_SUBTRAJ = Path(__file__).resolve().parent.parent.parent.parent / "RLSTCcode" / "subtrajcluster"
if _RLSTC_SUBTRAJ.exists() and str(_RLSTC_SUBTRAJ) not in sys.path:
    sys.path.insert(0, str(_RLSTC_SUBTRAJ))


def _load_pickle(path: str) -> Any:
    """Load a pickle file with Python 2/3 compatibility.

    Tries ``latin1`` encoding first (for Python-2 pickles), then ``bytes``.
    """
    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(f"Pickle file not found: {p}")

    for enc in ("latin1", "bytes"):
        try:
            with open(p, "rb") as f:
                return pickle.load(f, encoding=enc)
        except Exception:
            continue
    raise RuntimeError(f"Could not unpickle {p}")


def _convert_points(rlstc_points: list) -> List[Point]:
    """Convert RLSTCcode Point objects → Q-RLSTC Point dataclasses."""
    return [Point(x=float(p.x), y=float(p.y), t=float(p.t)) for p in rlstc_points]


def _convert_traj(rlstc_traj, traj_id: Optional[int] = None) -> Trajectory:
    """Convert a single RLSTCcode Traj → Q-RLSTC Trajectory."""
    points = _convert_points(rlstc_traj.points)
    tid = traj_id if traj_id is not None else getattr(rlstc_traj, "traj_id", None)
    return Trajectory(points=points, traj_id=tid)


# ---------------------------------------------------------------------------
# Public API
# ---------------------------------------------------------------------------

def load_trajectories(path: str, limit: Optional[int] = None) -> List[Trajectory]:
    """Load trajectories from a pickle file (e.g. ``Tdrive_norm_traj``).

    Args:
        path: Path to pickle file.
        limit: Optional max number of trajectories to load.

    Returns:
        List of Q-RLSTC Trajectory objects.
    """
    raw = _load_pickle(path)
    if limit is not None:
        raw = raw[:limit]
    return [_convert_traj(t, i) for i, t in enumerate(raw)]


def load_raw_trajectories(path: str, limit: Optional[int] = None) -> list:
    """Load trajectories as raw RLSTCcode Traj objects (for classical arm).

    Args:
        path: Path to pickle file.
        limit: Optional max number.

    Returns:
        List of original Traj objects (RLSTCcode format).
    """
    raw = _load_pickle(path)
    if limit is not None:
        raw = raw[:limit]
    return raw


def load_cluster_centers(path: str) -> Tuple[Dict[int, Any], float]:
    """Load cluster centers from a pickle file (e.g. ``tdrive_clustercenter``).

    The pickle format is::

        [(overall_sim, overall_sim, cluster_dict)]

    where ``cluster_dict[k]`` = ``[aver_dist, center_traj, temp_dists, segments]``.

    Args:
        path: Path to pickle file.

    Returns:
        (cluster_dict, baseline_od) where cluster_dict has Q-RLSTC Points.
    """
    raw = _load_pickle(path)
    overall_sim = float(raw[0][0])
    raw_cluster_dict = raw[0][2]

    cluster_dict: Dict[int, Any] = {}
    for k, v in raw_cluster_dict.items():
        aver_dist = v[0]
        center_traj = v[1]
        temp_dists = v[2]
        segments = v[3]

        center_points = _convert_points(center_traj.points)
        converted_segments = [_convert_traj(s) for s in segments] if segments else []

        cluster_dict[k] = {
            "aver_dist": aver_dist,
            "center": center_points,
            "temp_dists": temp_dists,
            "segments": converted_segments,
        }

    return cluster_dict, overall_sim


def load_cluster_centers_raw(path: str) -> Tuple[Dict, float]:
    """Load cluster centers in RLSTCcode's native dict format.

    Returns the dict in the exact format that MDP.py expects::

        cluster_dict[k] = [distances_list, segments_list, center_points,
                           defaultdict(list)]

    Args:
        path: Path to pickle file.

    Returns:
        (cluster_dict, baseline_od)
    """
    from collections import defaultdict

    raw = _load_pickle(path)
    overall_sim = float(raw[0][0])
    raw_cluster_dict = raw[0][2]

    cluster_dict: Dict[int, list] = {}
    for k, v in raw_cluster_dict.items():
        center_points = _convert_points(v[1].points)
        cluster_dict[k] = [
            [],              # [0]: distances
            [],              # [1]: segment trajs
            center_points,   # [2]: center points
            defaultdict(list),  # [3]: time-indexed points
        ]

    return cluster_dict, overall_sim


def load_subtrajectories(path: str) -> List[Trajectory]:
    """Load TRACLUS sub-trajectories (e.g. ``traclus_subtrajs``).

    These are the MDL-simplified sub-trajectories used for initial
    cluster center computation.
    """
    raw = _load_pickle(path)
    return [_convert_traj(t, i) for i, t in enumerate(raw)]


def load_test_set(path: str) -> List[Trajectory]:
    """Load a test/validation split (e.g. ``tdrive_testset0``).

    Same format as ``load_trajectories``.
    """
    return load_trajectories(path)


def get_data_dir() -> Path:
    """Return the path to q_rlstc's data directory."""
    return Path(__file__).resolve().parent.parent / "data"


def list_available_datasets() -> Dict[str, List[str]]:
    """List available pickle datasets in the data directory.

    Returns:
        Dict mapping dataset names to lists of available files.
    """
    data_dir = get_data_dir()
    if not data_dir.exists():
        return {}

    datasets: Dict[str, List[str]] = {
        "tdrive": [],
        "geolife": [],
        "other": [],
    }

    for f in sorted(data_dir.iterdir()):
        if f.is_file() and not f.name.endswith(".py") and not f.name.startswith("."):
            name = f.name.lower()
            if "tdrive" in name or "Tdrive" in f.name:
                datasets["tdrive"].append(f.name)
            elif "geolife" in name:
                datasets["geolife"].append(f.name)
            else:
                datasets["other"].append(f.name)

    return datasets
