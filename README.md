<p align="center">
  <img src="https://img.shields.io/badge/Qiskit-1.x-6929C4?logo=qiskit&logoColor=white" alt="Qiskit 1.x" />
  <img src="https://img.shields.io/badge/Python-3.10+-3776AB?logo=python&logoColor=white" alt="Python 3.10+" />
  <img src="https://img.shields.io/badge/License-MIT-green" alt="MIT License" />
  <img src="https://img.shields.io/badge/Platform-NISQ-orange" alt="NISQ Platform" />
</p>

# Q-RLSTC

**Quantum-Enhanced Reinforcement Learning for Sub-Trajectory Clustering**

A hybrid quantum-classical framework that replaces the classical Deep Q-Network in trajectory segmentation with a Variational Quantum Circuit â€” achieving **22Ã— parameter reduction** (20 vs. 450) while targeting comparable clustering quality on NISQ hardware.

---

## Why Q-RLSTC Exists

Sub-trajectory clustering groups portions of GPS trajectories that share similar movement patterns. Classical [RLSTC](https://github.com/llianga/RLSTCcode) solves this with a Deep Q-Network that learns _where_ to segment. Q-RLSTC asks: **can a 5-qubit quantum circuit learn the same policy with 95% fewer parameters?**

| | Classical RLSTC | Q-RLSTC |
|---|---|---|
| **Policy network** | Dense 5â†’64â†’2 MLP | 5-qubit VQ-DQN circuit |
| **Trainable parameters** | ~450 | 20 (Version A) / 32 (Version B) |
| **Optimizer** | SGD + backprop | SPSA (gradient-free) |
| **Hardware** | CPU / GPU | NISQ simulator (Aer) / IBM Quantum |

> **Honest caveat.** Q-RLSTC does not claim quantum speedup. The contribution is _parameter efficiency_ â€” demonstrating that a shallow quantum circuit can match a classical network on a real RL task â€” and a validated testbed for quantum RL research.

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Q-RLSTC System                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   CLASSICAL            â”‚   QUANTUM                                  â”‚
â”‚                        â”‚                                            â”‚
â”‚   Feature extraction   â”‚   VQ-DQN policy network                    â”‚
â”‚   Distance / OD proxy  â”‚   Angle encoding â†’ HEA â†’ Z-expectation    â”‚
â”‚   Reward computation   â”‚   SPSA parameter updates                   â”‚
â”‚   K-means evaluation   â”‚   (Optional) Swap test verification        â”‚
â”‚   Replay buffer        â”‚                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Only the policy network is quantum.** Everything else â€” features, distances, rewards, clustering â€” stays classical. This is deliberate: the policy has a fixed, low-dimensional I/O (5â†’2) that maps cleanly to qubits, while distance estimation demands incremental O(1) updates that quantum circuits cannot provide. See [Why classical vs. quantum](docs/wiki/justifications.md) for the full component-by-component analysis.

---

## Quick Start

```bash
# Install
cd q_rlstc
pip install -e ".[dev]"

# Run the synthetic demo
python experiments/run_synth_demo.py

# Run tests
pytest tests/ -v
```

---

## Documentation

> **ğŸ“– All deep-dive documents live in [`docs/wiki/`](docs/wiki/).** This README is the entry point.

| Document | What it covers |
|---|---|
| **[System Architecture](docs/wiki/architecture.md)** | Three-layer design, data flow, design philosophy, quantum scope boundary |
| **[MDP & Reward Engineering](docs/wiki/mdp_and_rewards.md)** | State space, action space, anti-gaming constraints, reward function design |
| **[Quantum Circuit Design](docs/wiki/quantum_circuit.md)** | Angle encoding, HEA ansatz, data re-uploading, Q-value extraction, Version A vs B circuits |
| **[Training Pipeline](docs/wiki/training_pipeline.md)** | SPSA optimizer, experience replay, Double DQN, target networks, hyperparameters |
| **[Distance & Clustering](docs/wiki/distance_and_clustering.md)** | IED metric, incremental computation, OD proxy, k-means, swap test (optional) |
| **[Classical vs. Quantum Justifications](docs/wiki/justifications.md)** | Component-by-component analysis: why each part is classical or quantum |
| **[RLSTC vs. Q-RLSTC Comparison](docs/wiki/comparison.md)** | Side-by-side technical comparison across 13 dimensions |
| **[Noise & Hardware Simulation](docs/wiki/noise_and_hardware.md)** | Backend factory, Eagle/Heron profiles, readout error mitigation |
| **[Experimental Design](docs/wiki/experimental_design.md)** | Cross-comparable baselines, metrics, experimental matrix |
| **[Debugging Guide](docs/wiki/debugging.md)** | Common failure modes, diagnostic functions, extension points |
| **[API Reference](docs/wiki/api_reference.md)** | Key classes, functions, and configuration dataclasses |

---

## Project Structure

```
q_rlstc/
â”œâ”€â”€ README.md                      â† You are here
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ q_rlstc/
â”‚   â”œâ”€â”€ config.py                  # All configuration dataclasses
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ features.py            # State feature extraction (Version A + B)
â”‚   â”‚   â””â”€â”€ synthetic.py           # Trajectory generation with ground truth
â”‚   â”œâ”€â”€ quantum/
â”‚   â”‚   â”œâ”€â”€ vqdqn_circuit.py       # VQ-DQN circuit builder
â”‚   â”‚   â”œâ”€â”€ backends.py            # Aer backend factory (ideal, Eagle, Heron)
â”‚   â”‚   â””â”€â”€ mitigation.py          # Readout error mitigation
â”‚   â”œâ”€â”€ rl/
â”‚   â”‚   â”œâ”€â”€ vqdqn_agent.py         # Agent wrapper (Îµ-greedy, target network)
â”‚   â”‚   â”œâ”€â”€ spsa.py                # SPSA optimizer
â”‚   â”‚   â”œâ”€â”€ train.py               # Training loop + MDP environment
â”‚   â”‚   â””â”€â”€ replay_buffer.py       # Experience replay buffer
â”‚   â””â”€â”€ clustering/
â”‚       â”œâ”€â”€ classical_kmeans.py    # K-means for episode-end evaluation
â”‚       â””â”€â”€ metrics.py             # OD, silhouette, F1 metrics
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ run_synth_demo.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_angle_encoding.py
â”‚   â”œâ”€â”€ test_hea_depth.py
â”‚   â”œâ”€â”€ test_kmeans_update.py
â”‚   â””â”€â”€ test_training_smoke.py
â””â”€â”€ docs/
    â””â”€â”€ wiki/                      # â† Deep-dive documentation
        â”œâ”€â”€ architecture.md
        â”œâ”€â”€ mdp_and_rewards.md
        â”œâ”€â”€ quantum_circuit.md
        â”œâ”€â”€ training_pipeline.md
        â”œâ”€â”€ distance_and_clustering.md
        â”œâ”€â”€ justifications.md
        â”œâ”€â”€ comparison.md
        â”œâ”€â”€ noise_and_hardware.md
        â”œâ”€â”€ experimental_design.md
        â”œâ”€â”€ debugging.md
        â””â”€â”€ api_reference.md
```

---

## Version A vs. Version B

Q-RLSTC ships with two configurations to answer different research questions:

| | Version A â€” _Close Comparison_ | Version B â€” _Quantum-Native_ |
|---|---|---|
| **Purpose** | Controlled experiment: isolate the effect of switching MLP â†’ VQC | Explore whether a larger Hilbert space yields better policies |
| **Qubits** | 5 | 8 |
| **State features** | 5 (same dimensionality as classical RLSTC) | 8 (adds `angle_spread`, `curvature_gradient`, `segment_density`) |
| **Trainable params** | 20 | 32 |
| **Readout** | `âŸ¨Zâ‚€âŸ©`, `âŸ¨Zâ‚âŸ©` | `wâ‚€âŸ¨Zâ‚€âŸ© + wâ‚âŸ¨Zâ‚‚Zâ‚ƒâŸ©`, `wâ‚‚âŸ¨Zâ‚âŸ© + wâ‚ƒâŸ¨Zâ‚„Zâ‚…âŸ©` |
| **Config** | `QRLSTCConfig(version="A")` | `QRLSTCConfig(version="B")` |

```python
from q_rlstc.config import QRLSTCConfig

# Scientific control â€” matches classical RLSTC dimensions
config_a = QRLSTCConfig(version="A")  # 5 qubits, 20 params

# Quantum-native â€” exploits larger Hilbert space
config_b = QRLSTCConfig(version="B")  # 8 qubits, 32 params
```

See [RLSTC vs. Q-RLSTC Comparison](docs/wiki/comparison.md) for the full breakdown across 13 dimensions.

---

## Key Design Decisions

| Decision | Choice | Rationale | Deep dive |
|---|---|---|---|
| Quantum scope | Policy network only | Small fixed I/O (5â†’2); distance needs O(1) incremental updates | [Justifications](docs/wiki/justifications.md) |
| Encoding | Angle (RY) | 1 feature â†’ 1 qubit; bounded via `arctan`; no normalization needed | [Circuit Design](docs/wiki/quantum_circuit.md) |
| Ansatz | HEA (RY-RZ + linear CNOT) | NISQ-friendly; sufficient expressivity for 5D state | [Circuit Design](docs/wiki/quantum_circuit.md) |
| Optimizer | SPSA | 2 evals per step vs. 40 for parameter-shift; shot-noise robust | [Training Pipeline](docs/wiki/training_pipeline.md) |
| Reward | OD Î” + boundary sharpness âˆ’ segment penalty | Markov-safe; dense signal on both EXTEND and CUT actions | [MDP & Rewards](docs/wiki/mdp_and_rewards.md) |
| Target network | Double DQN | Prevents Q-value overestimation | [Training Pipeline](docs/wiki/training_pipeline.md) |

---

## Comparative Systems

| System | Domain | Quantum Component | Relationship to Q-RLSTC |
|---|---|---|---|
| **RLSTCcode** | Trajectory clustering | None (classical MLP) | Direct predecessor; same MDP, different approximator |
| **TheFinalQRLSTC** | Trajectory clustering | VQ-DQN | Earlier prototype; Q-RLSTC is the modular rewrite |
| **qDINA** | Database indexing | BQN / TwoLocal | Similar quantum RL; SPSA-only, larger action space |
| **qmeans** | General clustering | Swap test | Unsupervised; shares amplitude encoding patterns |

See [Comparison](docs/wiki/comparison.md) for the full matrix.

---

## NISQ Constraints

| Constraint | Value | Rationale |
|---|---|---|
| Qubits | 5 (Version A) / 8 (Version B) | Matches state dimensionality |
| Circuit depth | ~11 layers | Below decoherence threshold for Eagle/Heron |
| Trainable parameters | 20 / 32 | Below barren plateau threshold |
| Shots (training) | 512 | Balance noise vs. iteration speed |
| Shots (evaluation) | 1024 | Lower variance for metric reporting |
| Entanglement | Linear CNOT chain | Fewer 2-qubit gates = less noise accumulation |

---

## References

1. Liang et al. â€” "Sub-trajectory clustering with deep reinforcement learning"
2. Chen et al. â€” "Variational Quantum Circuits for Deep Reinforcement Learning"
3. Schuld et al. â€” "Evaluating analytic gradients on quantum hardware"
4. PÃ©rez-Salinas et al. â€” "Data re-uploading for a universal quantum classifier"
5. Spall, J.C. â€” "Implementation of the Simultaneous Perturbation Algorithm for Stochastic Optimization" (SPSA)

---

## License

MIT License â€” Research code for academic use.
